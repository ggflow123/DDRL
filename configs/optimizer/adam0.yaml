optimizer_class: torch.optim.Adam
optimizer_kwargs:
  lr: 0.005
  betas: [0.9, 0.999]
  eps: 1e-08
  weight_decay: 0
  amsgrad: False
save_name: adam0
